{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ANN Example"
      ],
      "metadata": {
        "id": "1G866jlcGG0L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cuda\n",
        "yapay zeka tarafında hızlandırılmasını sağlamak için cuDNN\n",
        "deep neural networks \\\n",
        "matmul matriks multipication --> matrikslerin çarpımı \\\n",
        "pooling --> havuzlama \\\n",
        "normalization --> artificial neural networkte olan \\\n",
        "\n",
        "Normalizasyon kategorisi \\\n",
        "\n",
        "forward  --> ileri akış\\\n",
        "backward --> geriye akış\\\n",
        "attention --> ann tabanlı başka bir algoritma \\\n"
      ],
      "metadata": {
        "id": "RapCSxtXsh3s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ML İstatistiksel algoritmalarla çalışır. Hesaplanan, tasarlanan öğrenme süreci mevcuttur.\n",
        "Temelindeki istatistik algoritmaları her senaryoda işe yaramaz. Video, ses, görsel verilerde tamamen istatistikle yapılamaz.\n",
        "Complex veri olduğu için.\n",
        "\n",
        "Pattern çıkaramadığın bir şeyde istatistiki bir yaklaşım oluşturamazsın.\n",
        "\n",
        "istatisti\n",
        "deep neural network yapay sinir ağları black box'tır.\n",
        "sebebi bu algoritmaların öğrenme süreçlerini, patternleri, çıkarımları kendileri yapıyor olmalarıdır."
      ],
      "metadata": {
        "id": "QJZvCIitxAga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "DNNS genel kategorisi\n",
        "ANN çekirdeği persptro\n",
        "\n",
        "bir den fazla hidden layer varsa\n",
        "buna deep neural network deniyor.\n",
        "hiddenların sayısı ne kadar artarsa o kadar deep learning olur.\n",
        "\n",
        "tek bir hidden layer varsa ann\n"
      ],
      "metadata": {
        "id": "ftCtWKSTyuHo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gray Scale = 1 \\\n",
        "Renkli olursa = RGB = 3 katmanlı red, green, blue"
      ],
      "metadata": {
        "id": "9z_O-cWl-NEK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convelation katmanı,\n",
        "reducing azaltma, boyut azaltma işlemi gerçekleştirir. bu sayede veri üzerinde daha hızlı hesaplama gerçekleştirir.\n",
        "\n",
        "parametrelerin sayısını azaltıp doğruluğu arttırma işlemi gerçekleştiren yapılar gelişti.\n",
        "Bu sebeple ChatGPT3 175 milyarken ChatGPT 1.5 milyar parametreden oluşmaktadır.\n",
        "\n",
        "--------------\n",
        "\n",
        "CNN görsel veriler üzerinde daha hızlı ve iyi çalışır.\n",
        "\n",
        "ANN yine görsel veri üzerinde çalışır ancak daha yüksek sayıda parametre üretir. Parametre sayısı fazla olduğundan daha uzun ve maliyetli sürer ve doğruluk oranı daha düşüktür.\n",
        "\n",
        "cnn convelation katmanı olduğu için optimizasyon yapar.\n",
        "\n",
        "daha az parametreyle daha iyi sonuç üretir."
      ],
      "metadata": {
        "id": "_mgb0sIUCdR5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Veri seti : MNIST\n",
        "- Size of Image : 28x28 pixells with 0 to 255."
      ],
      "metadata": {
        "id": "upl63AI6D1jJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lI-1iuWftb5j"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model, layers\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Layer\n",
        "## 2 şer katlar şeklinde artmak ve katlar sayısı yeterli olacak ise yarı yarıya inecek şekilde nöron yapısını ayarlamak gerekmektedir.\n"
      ],
      "metadata": {
        "id": "w5Xw66FNbVYj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset parameters\n",
        "\n",
        "num_class = 10 # total classes (0-9 digits)\n",
        "num_features = 784 # data features (img shape = 28*28)\n",
        "# görsel veri üzerinde çalışabilmem için düzleştirme(flattening) yapmam gerekir. görsel boyut enlem*boylam olarak düzleştirilir.\n",
        "\n",
        "# Training Parameters.\n",
        "learning_rate = 0.1\n",
        "training_steps = 2000\n",
        "batch_size = 256\n",
        "display_step = 100\n",
        "\n",
        "#Network parameters\n",
        "n_hidden_1 = 128 # 1st layer number of neurans.\n",
        "n_hidden_2 = 256 # 2st layer number of neurans.\n",
        "\n",
        "# Research :\n",
        "\n",
        "  # Hiper parametreler nelerdir?\n",
        "  ##Eğitim başlamadan önce, değerlerine karar verdiğimiz veya konfigürasyonunu seçtiğiniz ve eğitim bittiğinde değerleri veya konfigürasyonu aynı kalacak herşey bir hiperparametredir.\n",
        "\n",
        "  # Learning Rate nedir?\n",
        "  ## Yüksek olursa 0.5 gibi topu yuvarladıni yüksekten bıraktın, 5m sonra yine durdu, 10m sonra yine yere vurdu gibi araları büyük olacaktır.\n",
        "  ## Learning rate yüksek olursa öğrenme hızı yükselir ancak model veriyi iyi öğrenemez.\n",
        "  ## Learning rate düşük olursa öğrenme hızı çok düşer. ve modelin veriyi iyi öğreneceğini garanti etmez.\n",
        "  ## Tavsiye 0.1, 0.001\n",
        "  ## Learning rate modelin eğitim hızını belirler. Düşük olursa çok yavaş öğrenir ve istediğimiz sonucu elde edemeyebiliriz. Yüksek olursa hızlı öğrenir eğitimi tam anlayamayabilir.\n",
        "  ## Kontrollü gitmek gerekir.\n",
        "\n",
        "\n",
        "  # Batch ve Batch size nedir?\n",
        "\n",
        "  #Dropout ? regularization yöntemlerinden birisidir.\n",
        "  ##Drop out -- her bir katmandaki nöronları rastgele seçerek disable ediyor. Böylece bir sonraki katmana aynı ağarlık gitmiyor. Bu da ezberleme oranını iyi bir şekilde düşürüyor.\n",
        "  ##Learning process içerisinde disable ediliyor. Bir sonraki çalışmada başka bir nöron disable edilebilir. Rastgelelik geliyor.\n",
        "  ## Dropout katmanları, sinir ağlarının aşırı öğrenmesini azaltmak için uygulanan bir yöntemdir.\n",
        "  ## %20 dropout değeri\n",
        "\n",
        "\n",
        "  #Reguliarisation tecniques\n",
        "\n",
        "  # Normalization (techniques) in deep learning?\n",
        "  # görsel veri seti nasıl hazırlanır?\n",
        "  # Data pre-processing nedir?\n",
        "\n",
        "  #Shuffling (in deep learning) nedir?\n",
        "  #Overfitting ve underfitting nedir?\n",
        "  #overgittingden korunma yöntemleri nelerdir?\n",
        "\n",
        "  #Forward-propagation nedir?\n",
        "  #backward-propagation nedir?\n",
        "  #chain rule (backprop) nedir?\n",
        "  #activation function nedir ve çeşitleri nelerdir?\n",
        "\n",
        "  #loss function nedir? (aka. loss, cost, error)\n",
        "  #optimizer (in deep learning) nedir?\n",
        "\n",
        "  #gradient descent nedir?\n",
        "  ## Optimizasyon algoritmalarından bir  tanesidir. Optimization algorithms başlığı içerisinde yer alır.\n",
        "  ## ML modelinizi eğitmek için kullanılan optimizasyon algoritmasıdır. Dışbükey bir fonksiyona dayanır ve belirli bir fonksiyonu local minumuma indirmek için parametreleri otomatik olarak ayarlar.\n",
        "  ## Differentiable functionnın Local minimumu bulmak için kullanılan bir optimizasyon algoritması.\n",
        "  ## Makine öğrenmesindeki GD Bir maliyet fonksiyonunu (lost, cost, error func) mümkün olduğu kadar en aza indiren bir fonksiyonun parametrelerin değerini bulmak için kullanılır.\n",
        "  ## Parametre değerlerini tanımlayarak başlarsınız. Ardından paramet\n",
        "  ## gradient: girdileri biraz değiştirirseniz bir fonksiyonun çıktısını ne kadar değişeceğini ölçer.\n",
        "  ## Hatadaki değişime göre tüm ağırlıkların değişimi ölçer. Gradyanı bir fonksiyonun eğitimi olarak da düşünebilirsiniz.\n",
        "  ##Eğim ne kadar yüksek olursa eğim o kadar dik olur ve model o kadar hızlı öğrenebilir.\n",
        "  ##local minumum: olabilecek minimum öğrenme aşamasına getirebilmek.\n",
        "  ##Gradient, GradientDescent ve Learning Rate birbiriyle bağlantılıdır.\n",
        "  ## gradient autodiff sayesinde hesaplanır.\n",
        "  ##Feed forward sırasında bütün işlemleri hatırlaması gerekiyor ki bizim autodiff hesaplamalarımızı yapabilsin.\n",
        "  ##Autodiff ağırlıkların güncellenmesi işlemini gerçekleştirir. feedforwarddaki tüm opları hatırlıyor.\n",
        "  ##Sonra ağırlıkları güncellemek için hatırladığı işlemleri tersine doğru ağırlıklarını güncellemek için geriye doğru çalışır.\n",
        "  ## Feed forward kasete kaydeder. kayıtlı olaran gradientleri hesaplamak için\n",
        "  ## Kaydedilmiş gradientleri hesaplamak için reverse mode differentiation kullanır.\n",
        "  ## gradientTape feed forward aşamasında her şeyi kaydeder. Ardından x'den y'ye giderken kullanılan bilgileri yeniden hesaplar.\n",
        "\n",
        "  ##with akan bir veri demektir. GradientTape açılıp kapanabilmesi gereken bir kaynaktır. GradientTape bir kayıt açar ve yapılan işlemleri kaydeder ve kayıt bitince yapılan işlemler üzerinde hesaplama yapar.\n",
        "  ##\n",
        "\n",
        "  ##back prop aşamasında tensorflow bu sırada gradientleri hesaplamak için ters sırada dolaşır.\n",
        "\n",
        "  #GradientTape\n",
        "  ##\n",
        "  ## Autodiff, bir fonksiyonun gradyanlarını girişlerine göre hesaplama işlemidir.\n",
        "  ## Gradient öyle bir şey ki, bir hesap yapıp inputu ne yaparsam networkun çıktısı ne oluru hesaplıyordu.\n",
        "  ## Girişlerine göre hesaplanmasını gerçekleştiren şey automatic differentiation. Gradyanlar bir fonksiyonun eğimleri ya da değişim oranlarıdır.\n",
        "  ## Tensorflow bir gradienttape örneği kapsamında gerçekleştirilen işlemleri takip ederek bunları bir kaset üzerine kaydeder.\n",
        "  ##\n",
        "\n",
        "  ##Eğitilebilen veriler üzerinde hesaplama yapılır.\n",
        "  ##eğitilemez olarak tanımlarsan değişkeni gradientTape takip etmez.\n",
        "\n",
        "\n",
        "  ## Early stop\n",
        "  ## Loss function'ın normalde düşürmeyi amaçlarız. Ancak eğitim süresinde yükselmeye başlayınca eğitimi durdurabiliriz.\n",
        "  ## loss function'ın passions olarak belirttiğimiz rakam kadar yükselirse early stopping gerçekleşecek. Yani 5 passion olarak belirledik.\n",
        "  ## loss function 5 kere yükselirse early stop olur.\n",
        "  ## 5 loss artmadan önceki ilk model en iyi modeldir mantığı vardır.\n",
        "\n",
        "  #gradient descent ve gradientTape ilişkisi. GradientTape nedir ve ne işe yarar? (kaynak: tf dokümantasyonu)\n",
        "  #\n",
        "\n",
        "  # Validation nedir? Test nedir? ikisi arasındaki farklar...\n",
        "\n",
        "  ## Hatayı hesaplarken ağırlıkları en optimum şekilde\n",
        "\n",
        "  ## Trainable - non trainable parametreler:\n",
        "  ## training sırasında ağırlıkları güncellenebilen parametrelere trainable parametreler denir.\n",
        "  ##training yapılırken güncellenemeyen parametrelere non trainable parametreler denir.\n",
        "\n",
        "  ##convolition katmanındaki parametreler nontrainable parametrelerdir. Amaçları öğrenmek değildir çünkü\n",
        "  ##CNN'i 2 ye böldük, convolutional katmanı ve Artificial Neural Network olarak. veriyi convolutional katmanı indirgedi ve ANN katmanına gönderdi. ANN katmanı da veriyi öğrenir.\n",
        "\n",
        "  ### Hidden layerlarda son 1-2 yıldır Activation function olarak daha yüksek doğruluk verdiği için RELU kullanılır.\n",
        "  #### Algoritma tabanında (Multi neural network), multi-task learning deep learning models.\n",
        "  ### Adam optimizer en çok kullanılan optimizer'dıdr.\n",
        "\n",
        "\n",
        "#Yapı\n",
        "  ## birden fazla taski yapabilen neural network arthitecture'ı geliştircek. ios üzerinden zararlı, android üzerinden zararlı yazılımlar üretme vb. gibi.\n",
        "  ## paylaşımlı bir hidden layerlar olsun, hepsi bunun devamı olarak ayrışsın demişler.\n",
        "\n",
        "  #Parametreler\n",
        "  ## eğitilebilir-eğitilemez parametreler olarak 2'ye ayrılır. Convolution Katmanındaki parametreler non trainabel, hidden layerdaki parametreler trainabel parametrelerdir.\n",
        "  ## eğitilemeyen parametreler, optimizasyon olmadığı için modeli daha hızlı çalıştıracaktır.\n",
        "  ## ancak eğitilemediği için optimizasyon olmayacağından model iyi öğrenemeyedebilir.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1XnJ6MeG8pxz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veri üzerinde karıştırma yapılarak Overfitting'in önüne geçilebilir."
      ],
      "metadata": {
        "id": "HnJLJbQCK44Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare Dataset\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# print(\"Before converting\",type(x_train))\n",
        "#Convert to float32 ---> aşağıdaki dönüşümleri yapabilmek için float32 ye çevirdik.\n",
        "x_train, x_test = np.array(x_train, np.float32), np.array(x_test, np.float32)\n",
        "# print(type(x_train))\n",
        "\n",
        "\n",
        "#Flatten images to 1-D vector of 784 feature (28*28)\n",
        "x_train, x_test = x_train.reshape([-1, num_features]), x_test.reshape([-1, num_features])\n",
        "#Normalize images value from [0, 255] to [0,1].\n",
        "x_train, x_test = x_train /255, x_test/255"
      ],
      "metadata": {
        "id": "rprctjEHIFA_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use tf.data API to shuffle and batch data.\n",
        "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_data = train_data.repeat().shuffle(5000).batch(batch_size).prefetch(1)"
      ],
      "metadata": {
        "id": "K_fi9pqxJQCN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "prefetch araştır."
      ],
      "metadata": {
        "id": "LEEBw1TOLVWf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Research:\n",
        "  ### OOP in Python (Inheritance - Kalıtım)\n",
        "  ### super() class / function\n",
        "  ### Python __init__\n",
        "\n",
        "  başlıklar"
      ],
      "metadata": {
        "id": "CNCRiejOLuwE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### activation = input-output arasında veri akışı var. Bu akışta yetersiz de bir çok veri var.\n",
        "### Neural Networkune değer katabilecek büyüklükte/değerinde değil. Değeri yeterli olmayan verilerin diskalifiye olması gerekiyor.\n",
        "###Verilerin bellli bir olgunlukta olması gerekiyor. Bu olgunluğa karar veren ise activation function'dır.\n",
        "###Gelen veri activation function'ımı ateşlemeye yeterli midir.\n",
        "  ## bias = öğrenmeyi sağlayabilmek için rastgele eklediğim bir değerdir.\n",
        "### Bir sonraki katmana aktarabileceğim kadar yeterli mi bunu test ediyor\n",
        "### her bir neuron'un sonunda bunu denetleyebilmek için bir activation function vardır.\n",
        "\n",
        "### Feedforward --> ileri doğru bilgi akışı. Sağdan sola doğru öğrenerek ilerlenen öğrenme türüdür.\n",
        "### Backpropagation --> hata fonksiyonu oluşturulduktan sonra, daha iyi öğrenebilmesi için geriye dönüp ağırlıkları güncellemek ile birlikte optimizasyon yapma sürecidir.\n",
        "###Geriye doğru gitmek gerekiyor ancak. Zinciri geriye doğru takip ederek ağırlıklar güncellenir.\n",
        "### chain rule in neural network\n",
        "\n",
        "\n",
        "### cost/error/lost function = hata hesaplamasına yarıyor.\n"
      ],
      "metadata": {
        "id": "vQhugMaSi28l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dense'i fully connect yapabilmek için kullanılır.\n",
        "\n",
        "class NeuralNet(Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(NeuralNet, self).__init__()\n",
        "    self.fc1 = layers.Dense(n_hidden_1, activation = tf.nn.relu) # First fully-connected hidden layer\n",
        "    self.fc2 = layers.Dense(n_hidden_2, activation = tf.nn.relu) # Second fully-connected hidden layer\n",
        "    self.out = layers.Dense(num_class)\n",
        "\n",
        "  #Forward pass\n",
        "  def call(self, x, is_training=False):\n",
        "    x = self.fc1(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.out(x)\n",
        "    if not is_training:\n",
        "      #tf cross emtropy expect logit without softmax, so only apply when not training.\n",
        "      x = tf.nn.sıftmax(x)\n",
        "      return x"
      ],
      "metadata": {
        "id": "b2zEdvU0LUUQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# activation = input-output arasında veri akışı var. Bu akışta yetersiz de bir çok veri var.\n",
        "# Neural Networkune değer katabilecek büyüklükte/değerinde değil. Değeri yeterli olmayan verilerin diskalifiye olması gerekiyor.\n",
        "#Verilerin bellli bir olgunlukta olması gerekiyor. Bu olgunluğa karar veren ise activation function'dır.\n",
        "#Gelen veri activation function'ımı ateşlemeye yeterli midir.\n",
        "  ## bias = öğrenmeyi sağlayabilmek için rastgele eklediğim bir değerdir.\n",
        "# Bir sonraki katmana aktarabileceğim kadar yeterli mi bunu test ediyor\n",
        "# her bir neuron'un sonunda bunu denetleyebilmek için bir activation function vardır.\n",
        "\n",
        "# Feedforward --> ileri doğru bilgi akışı. Sağdan sola doğru öğrenerek ilerlenen öğrenme türüdür.\n",
        "# Backpropagation --> hata fonksiyonu oluşturulduktan sonra, daha iyi öğrenebilmesi için geriye dönüp ağırlıkları güncellemek ile birlikte optimizasyon yapma sürecidir.\n",
        "#Geriye doğru gitmek gerekiyor ancak. Zinciri geriye doğru takip ederek ağırlıklar güncellenir.\n",
        "\n",
        "# back propagation = ağırlıkları ve bias'ı optimize ederek cost function'ı minimize etmeyi amaçlar\n",
        "\n",
        "# cost/error/lost function = hata hesaplamasına yarıyor.\n",
        "\n",
        "### chain rule in neural network araştır\n",
        "\n",
        "\n",
        "#aktivasyon fonksiyonu modelimizi ileri düzeye getiren şeydir.\n",
        "# 10 tane non-linear activation function var.\n",
        "#pozitif bir değer alırsa 0 ya da 1 değerini üretir."
      ],
      "metadata": {
        "id": "KNy9kFsAkxTJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "neural_net = NeuralNet()"
      ],
      "metadata": {
        "id": "DO0fMMvhkzCB"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross- Entropy Loss\n",
        "# Not that this will apply 'softmax' to the logits\n",
        "\n",
        "def cross_entropy_loss(x, y):\n",
        "  # Convert labels to int64 for tf cross-entropy function.\n",
        "  y = tf.cast(y, tf.int64)\n",
        "  # Apply softmax to logits and compute cross-entropy\n",
        "  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=x)\n",
        "  #Average loss across the batch\n",
        "  return tf.reduce_mean(loss)\n",
        "\n",
        "#Accuracy Metric.\n",
        "def accuracy(y_pred, y_true):\n",
        "  # Predict class is the index of higest score in prediction vector(i.e. argmax).\n",
        "  correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))\n",
        "  return tf.reduce_mean(tf.cast(correct_prediction, tf.float32), axis = -1)\n",
        "\n",
        "optimizer = tf.optimizers.SGD(learning_rate)"
      ],
      "metadata": {
        "id": "oJhiUiP2lsJg"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimization Process\n",
        "def run_optimization(x, y):\n",
        "\n",
        "  #wrap computation inside a GradientTape for automatic differantiation.\n",
        "  with tf.GradientTape() as g:\n",
        "    #Forward pass.\n",
        "    pred = neural_net(x, is_training=True)\n",
        "    #Compute loss.\n",
        "    loss = cross_entropy_loss(pred, y)\n",
        "\n",
        "\n",
        "  #Variable to update, i.e. trainable variables.\n",
        "  trainable_variables = neural_net.trainable_variables\n",
        "\n",
        "  #Compute gradients.\n",
        "  gradients = g.gradient(loss, trainable_variables)\n",
        "\n",
        "  #Update W and b following gradients.\n",
        "  optimizer.apply_gradients(zip(gradients, trainable_variables))"
      ],
      "metadata": {
        "id": "xf9CRSHPmQVB"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run training for the given number of steps.\n",
        "for step, (batch_x, batch_y) in enumerate(train_data.take(training_steps), 1):\n",
        "\n",
        "  run_optimization(batch_x, batch_y)\n",
        "  # Run the optimization to update W and b values.\n",
        "\n",
        "  if step % display_step == 0:\n",
        "    pred = neural_net(batch_x, is_training=True)\n",
        "    loss = cross_entropy_loss(pred, batch_y)\n",
        "    acc = accuracy(pred, batch_y)\n",
        "    print(\"Step: %i, Loss: %f, Accuracy: %f\" % (step, loss, acc))"
      ],
      "metadata": {
        "id": "ztET0nF0wn2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uWSh76hUqpDr"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}